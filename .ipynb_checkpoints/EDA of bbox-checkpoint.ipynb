{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchor Boxes Analysis using K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "C extension: No module named 'pandas._libs.tslib' not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace --force' to build the C extensions first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     from pandas._libs import (hashtable as _hashtable,\n\u001b[0m\u001b[0;32m     27\u001b[0m                              \u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtslib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0miNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimedelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'pandas._libs.tslib'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-aa6e154b839b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                       \u001b[1;34m\"pandas from the source directory, you may need to run \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                       \u001b[1;34m\"'python setup.py build_ext --inplace --force' to build \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                       \"the C extensions first.\".format(module))\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: C extension: No module named 'pandas._libs.tslib' not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace --force' to build the C extensions first."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with Image resizing (correct way!)\n",
    "\n",
    "#### Read csv\n",
    "A csv was made from json file using \"json_to_csv.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the dataset\n",
    "data = pd.read_csv('all_labels.csv')\n",
    "\n",
    "#utility function to get width & height\n",
    "def change_to_wh (data):\n",
    "    data['w'] = data['xmax'] - data['xmin'] + 1\n",
    "    data['h'] = data['ymax'] - data['ymin'] + 1\n",
    "    return data\n",
    "\n",
    "min_dimension = 480\n",
    "max_dimension = 720\n",
    "\n",
    "# function from Tensorflow Object Detection API to resize image\n",
    "def _compute_new_static_size(width, height, min_dimension, max_dimension):\n",
    "    orig_height = height\n",
    "    orig_width = width\n",
    "    orig_min_dim = min(orig_height, orig_width)\n",
    "  \n",
    "    # Calculates the larger of the possible sizes\n",
    "    large_scale_factor = min_dimension / float(orig_min_dim)\n",
    "      # Scaling orig_(height|width) by large_scale_factor will make the smaller\n",
    "      # dimension equal to min_dimension, save for floating point rounding errors.\n",
    "      # For reasonably-sized images, taking the nearest integer will reliably\n",
    "      # eliminate this error.\n",
    "    large_height = int(round(orig_height * large_scale_factor))\n",
    "    large_width = int(round(orig_width * large_scale_factor))\n",
    "    large_size = [large_height, large_width]\n",
    "    if max_dimension:\n",
    "    # Calculates the smaller of the possible sizes, use that if the larger\n",
    "    # is too big.\n",
    "        orig_max_dim = max(orig_height, orig_width)\n",
    "        small_scale_factor = max_dimension / float(orig_max_dim)\n",
    "    # Scaling orig_(height|width) by small_scale_factor will make the larger\n",
    "    # dimension equal to max_dimension, save for floating point rounding\n",
    "    # errors. For reasonably-sized images, taking the nearest integer will\n",
    "    # reliably eliminate this error.\n",
    "        small_height = int(round(orig_height * small_scale_factor))\n",
    "        small_width = int(round(orig_width * small_scale_factor))\n",
    "        small_size = [small_height, small_width]\n",
    "        new_size = large_size\n",
    "    if max(large_size) > max_dimension:\n",
    "        new_size = small_size\n",
    "    else:\n",
    "        new_size = large_size\n",
    "    \n",
    "    return new_size[1], new_size[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = change_to_wh(data)\n",
    "data['new_w'], data['new_h'] = np.vectorize(_compute_new_static_size)(data['width'], \n",
    "                                                                      data['height'], min_dimension, max_dimension)\n",
    "data['b_w'] = data['new_w']*data['w']/data['width']\n",
    "data['b_h'] = data['new_h']*data['h']/data['height']\n",
    "data['b_ar'] = data['b_w']/data['b_h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joint Plot of b_w and b_h\n",
    "sns.jointplot(x=\"b_w\", y=\"b_h\", data=data)\n",
    "sns.jointplot(x=\"b_w\", y=\"b_h\", data=data, kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the base box size!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_base_size(width, height, input_array=[64,96,128,196,212,256,512]):\n",
    "def count_base_size(width, height, input_array=[32, 64, 128, 256, 512]):\n",
    "    result = {}\n",
    "    for ele in input_array:\n",
    "        result[str(ele)] = 0\n",
    "    result['rest'] = 0\n",
    "    \n",
    "#     import itertools\n",
    "#     for w,h in itertools.izip(width,height):\n",
    "    for w,h in zip(width,height):\n",
    "        done = False\n",
    "        for inp in input_array:\n",
    "            if w <= inp and h <= inp:\n",
    "                result[str(inp)] += 1\n",
    "                done = True\n",
    "        if done == False:\n",
    "            result['rest'] += 1\n",
    "            \n",
    "    return result\n",
    "    \n",
    "D = count_base_size(data[\"b_w\"].tolist(), data[\"b_h\"].tolist())\n",
    "import collections\n",
    "OD = collections.OrderedDict(sorted(D.items()))\n",
    "plt.bar(range(len(OD)), OD.values(), align='center')  # python 2.x\n",
    "plt.xticks(range(len(OD)), OD.keys())  # in python 2.x\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area Scale\n",
    "base_box = 256*256\n",
    "data['b_area_scale'] = (data['w']*data['h']/(base_box)).apply(np.sqrt)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF scale\n",
    "base_anchor = 64\n",
    "data['tf_scale'] = data['b_h']*(data['b_ar']).apply(np.sqrt)/base_anchor\n",
    "data['tf_scale_2']= data['b_w']/((data['b_ar']).apply(np.sqrt)*base_anchor)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterring\n",
    "\n",
    "### Eucledian Distance Based Clusterring\n",
    "\n",
    "### One by One (AR and Scale Separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aspect Ratio\n",
    "X = data.iloc[:,13].values  # 'b_ar'\n",
    "X = X.reshape(-1,1)\n",
    "\n",
    "\n",
    "loss = []\n",
    "from sklearn.cluster import KMeans\n",
    "for i in range(2,10):\n",
    "    K = KMeans(i, random_state=1)\n",
    "    labels = K.fit(X)\n",
    "    loss.append(labels.inertia_)\n",
    "\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns[13]) # 'b_ar'\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "K = KMeans(4, random_state=1)\n",
    "labels = K.fit(X)\n",
    "print(\"Aspect Ratios:\\n\")\n",
    "print(labels.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Check (ignore!)\n",
    "\n",
    "x = data['w']/data['h']\n",
    "\n",
    "data['b_ar'].corr(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TF_Scale Clusterring\n",
    "X = data.iloc[:,15].values # tf_scale\n",
    "X = X.reshape(-1,1)\n",
    "\n",
    "\n",
    "loss = []\n",
    "from sklearn.cluster import KMeans\n",
    "for i in range(2,10):\n",
    "    K = KMeans(i, random_state=1)\n",
    "    labels = K.fit(X)\n",
    "    loss.append(labels.inertia_)\n",
    "\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of different scales\n",
    "plt.plot(X, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns[15]) # # tf_scale\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "K = KMeans(4, random_state=1)\n",
    "labels = K.fit(X)\n",
    "labels.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area_Scale\n",
    "X = data.iloc[:,15].values # # tf_scale\n",
    "X = X.reshape(-1,1)\n",
    "\n",
    "\n",
    "loss = []\n",
    "from sklearn.cluster import KMeans\n",
    "for i in range(2,10):\n",
    "    K = KMeans(i, random_state=1)\n",
    "    labels = K.fit(X)\n",
    "    loss.append(labels.inertia_)\n",
    "\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns[15]) # # tf_scale\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "K = KMeans(4, random_state=1)\n",
    "labels = K.fit(X)\n",
    "labels.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterring using both width & height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns[11:13]) # 'b_w', 'b_h'\n",
    "X = data.as_matrix(columns=data.columns[11:13])\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "K = KMeans(4, random_state=0)\n",
    "labels = K.fit(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels.labels_,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = labels.cluster_centers_\n",
    "\n",
    "ar = out[:,0]/out[:,1]\n",
    "scale = out[:,1]*np.sqrt(ar)/256\n",
    "\n",
    "print(\"Aspect Ratios: \")\n",
    "print(ar)\n",
    "\n",
    "print(\"Scales: \")\n",
    "print (scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOU based clusterring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility functions for K-means\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def iou(box, clusters):\n",
    "    \"\"\"\n",
    "    Calculates the Intersection over Union (IoU) between a box and k clusters.\n",
    "    :param box: tuple or array, shifted to the origin (i. e. width and height)\n",
    "    :param clusters: numpy array of shape (k, 2) where k is the number of clusters\n",
    "    :return: numpy array of shape (k, 0) where k is the number of clusters\n",
    "    \"\"\"\n",
    "    x = np.minimum(clusters[:, 0], box[0])\n",
    "    y = np.minimum(clusters[:, 1], box[1])\n",
    "    if np.count_nonzero(x == 0) > 0 or np.count_nonzero(y == 0) > 0:\n",
    "        raise ValueError(\"Box has no area\")\n",
    "\n",
    "    intersection = x * y\n",
    "    box_area = box[0] * box[1]\n",
    "    cluster_area = clusters[:, 0] * clusters[:, 1]\n",
    "\n",
    "    iou_ = intersection / (box_area + cluster_area - intersection)\n",
    "\n",
    "    return iou_\n",
    "\n",
    "\n",
    "def avg_iou(boxes, clusters):\n",
    "    \"\"\"\n",
    "    Calculates the average Intersection over Union (IoU) between a numpy array of boxes and k clusters.\n",
    "    :param boxes: numpy array of shape (r, 2), where r is the number of rows\n",
    "    :param clusters: numpy array of shape (k, 2) where k is the number of clusters\n",
    "    :return: average IoU as a single float\n",
    "    \"\"\"\n",
    "    return np.mean([np.max(iou(boxes[i], clusters)) for i in range(boxes.shape[0])])\n",
    "\n",
    "\n",
    "def translate_boxes(boxes):\n",
    "    \"\"\"\n",
    "    Translates all the boxes to the origin.\n",
    "    :param boxes: numpy array of shape (r, 4)\n",
    "    :return: numpy array of shape (r, 2)\n",
    "    \"\"\"\n",
    "    new_boxes = boxes.copy()\n",
    "    for row in range(new_boxes.shape[0]):\n",
    "        new_boxes[row][2] = np.abs(new_boxes[row][2] - new_boxes[row][0])\n",
    "        new_boxes[row][3] = np.abs(new_boxes[row][3] - new_boxes[row][1])\n",
    "    return np.delete(new_boxes, [0, 1], axis=1)\n",
    "\n",
    "\n",
    "def kmeans(boxes, k, dist=np.median):\n",
    "    \"\"\"\n",
    "    Calculates k-means clustering with the Intersection over Union (IoU) metric.\n",
    "    :param boxes: numpy array of shape (r, 2), where r is the number of rows\n",
    "    :param k: number of clusters\n",
    "    :param dist: distance function\n",
    "    :return: numpy array of shape (k, 2)\n",
    "    \"\"\"\n",
    "    rows = boxes.shape[0]\n",
    "\n",
    "    distances = np.empty((rows, k))\n",
    "    last_clusters = np.zeros((rows,))\n",
    "\n",
    "    np.random.seed()\n",
    "\n",
    "    # the Forgy method will fail if the whole array contains the same rows\n",
    "    clusters = boxes[np.random.choice(rows, k, replace=False)]\n",
    "\n",
    "    while True:\n",
    "        for row in range(rows):\n",
    "            distances[row] = 1 - iou(boxes[row], clusters)\n",
    "\n",
    "        nearest_clusters = np.argmin(distances, axis=1)\n",
    "\n",
    "        if (last_clusters == nearest_clusters).all():\n",
    "            break\n",
    "\n",
    "        for cluster in range(k):\n",
    "            clusters[cluster] = dist(boxes[nearest_clusters == cluster], axis=0)\n",
    "\n",
    "        last_clusters = nearest_clusters\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns[11:13])\n",
    "X = data.as_matrix(columns=data.columns[11:13]) # 'b_w', 'b_h'\n",
    "\n",
    "# Cluster with 4 centers\n",
    "cl = kmeans(X, 4)\n",
    "print(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_iou = cl[:,0]/cl[:,1]\n",
    "\n",
    "print(ar_iou)\n",
    "\n",
    "\n",
    "scale_iou = cl[:,1]*np.sqrt(ar_iou)/256\n",
    "print(scale_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
